{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/envs/torchrl_up/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from reward.utils import torch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import reward as rw\n",
    "import reward.utils as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = rw.envs.GymEnv(\"CartPole-v0\")\n",
    "runner = rw.runners.SingleRunner(env)\n",
    "batcher = rw.batchers.RolloutBatcher(runner=runner, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.activation = activation()\n",
    "        \n",
    "        self.hidden = nn.Linear(num_inputs, 64)\n",
    "        self.out = nn.Linear(64, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        return self.out(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNN(nn.Module):\n",
    "    def __init__(self, num_inputs, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.activation = activation()\n",
    "        \n",
    "        self.hidden = nn.Linear(num_inputs, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalPolicy(rw.policy.BasePolicy):\n",
    "    def create_dist(self, state):\n",
    "        logits = self.nn(state)\n",
    "        return rw.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "        dist = self.create_dist(state)\n",
    "        return U.to_np(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nn = PolicyNN(batcher.state_info.shape[0], batcher.action_info.shape).to(device)\n",
    "v_nn = ValueNN(batcher.state_info.shape[0]).to(device)\n",
    "policy = CategoricalPolicy(p_nn)\n",
    "\n",
    "p_opt = torch.optim.Adam(p_nn.parameters(), lr=1e-3)\n",
    "v_opt = torch.optim.Adam(v_nn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to: logs/cart_pole/v0-1\n"
     ]
    }
   ],
   "source": [
    "logger = U.Logger('logs/cart_pole/v0-1')\n",
    "last_logged_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9afffcbe6964673b32e64c5bd59232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=300000), HTML(value='')), layout=Layout(displâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                              |  25.63\n",
      "Env/Length/Episode (New)                              |  25.63\n",
      "Env/Reward/Episode (Last 50)                          |  25.63\n",
      "Env/Length/Episode (Last 50)                          |  25.63\n",
      "policy/loss                                           |  10.62\n",
      "v/loss                                                | 368.68\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                              |  30.22\n",
      "Env/Length/Episode (New)                              |  30.22\n",
      "Env/Reward/Episode (Last 50)                          |  34.80\n",
      "Env/Length/Episode (Last 50)                          |  34.80\n",
      "policy/loss                                           |  12.24\n",
      "v/loss                                                | 509.28\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                              |  40.15\n",
      "Env/Length/Episode (New)                              |  40.15\n",
      "Env/Reward/Episode (Last 50)                          |  48.16\n",
      "Env/Length/Episode (Last 50)                          |  48.16\n",
      "policy/loss                                           |  14.91\n",
      "v/loss                                                | 819.51\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |   50.52\n",
      "Env/Length/Episode (New)                             |   50.52\n",
      "Env/Reward/Episode (Last 50)                         |   54.26\n",
      "Env/Length/Episode (Last 50)                         |   54.26\n",
      "policy/loss                                          |   18.98\n",
      "v/loss                                               | 1295.06\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |   62.01\n",
      "Env/Length/Episode (New)                             |   62.01\n",
      "Env/Reward/Episode (Last 50)                         |   60.70\n",
      "Env/Length/Episode (Last 50)                         |   60.70\n",
      "policy/loss                                          |   16.40\n",
      "v/loss                                               | 1036.49\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |   80.49\n",
      "Env/Length/Episode (New)                             |   80.49\n",
      "Env/Reward/Episode (Last 50)                         |   86.20\n",
      "Env/Length/Episode (Last 50)                         |   86.20\n",
      "policy/loss                                          |   19.21\n",
      "v/loss                                               | 1451.95\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  106.42\n",
      "Env/Length/Episode (New)                             |  106.42\n",
      "Env/Reward/Episode (Last 50)                         |  105.78\n",
      "Env/Length/Episode (Last 50)                         |  105.78\n",
      "policy/loss                                          |   19.66\n",
      "v/loss                                               | 1499.87\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  121.77\n",
      "Env/Length/Episode (New)                             |  121.77\n",
      "Env/Reward/Episode (Last 50)                         |  123.36\n",
      "Env/Length/Episode (Last 50)                         |  123.36\n",
      "policy/loss                                          |   27.39\n",
      "v/loss                                               | 2617.24\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  134.14\n",
      "Env/Length/Episode (New)                             |  134.14\n",
      "Env/Reward/Episode (Last 50)                         |  139.34\n",
      "Env/Length/Episode (Last 50)                         |  139.34\n",
      "policy/loss                                          |   21.26\n",
      "v/loss                                               | 1716.74\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  154.61\n",
      "Env/Length/Episode (New)                             |  154.61\n",
      "Env/Reward/Episode (Last 50)                         |  157.40\n",
      "Env/Length/Episode (Last 50)                         |  157.40\n",
      "policy/loss                                          |   27.55\n",
      "v/loss                                               | 2791.64\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  162.73\n",
      "Env/Length/Episode (New)                             |  162.73\n",
      "Env/Reward/Episode (Last 50)                         |  159.86\n",
      "Env/Length/Episode (Last 50)                         |  159.86\n",
      "policy/loss                                          |   24.51\n",
      "v/loss                                               | 2346.47\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  173.83\n",
      "Env/Length/Episode (New)                             |  173.83\n",
      "Env/Reward/Episode (Last 50)                         |  178.48\n",
      "Env/Length/Episode (Last 50)                         |  178.48\n",
      "policy/loss                                          |   25.12\n",
      "v/loss                                               | 2482.72\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  163.87\n",
      "Env/Length/Episode (New)                             |  163.87\n",
      "Env/Reward/Episode (Last 50)                         |  159.96\n",
      "Env/Length/Episode (Last 50)                         |  159.96\n",
      "policy/loss                                          |   24.74\n",
      "v/loss                                               | 2312.15\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  165.55\n",
      "Env/Length/Episode (New)                             |  165.55\n",
      "Env/Reward/Episode (Last 50)                         |  168.86\n",
      "Env/Length/Episode (Last 50)                         |  168.86\n",
      "policy/loss                                          |   24.45\n",
      "v/loss                                               | 2303.91\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  180.81\n",
      "Env/Length/Episode (New)                             |  180.81\n",
      "Env/Reward/Episode (Last 50)                         |  180.60\n",
      "Env/Length/Episode (Last 50)                         |  180.60\n",
      "policy/loss                                          |   22.00\n",
      "v/loss                                               | 2102.20\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  187.91\n",
      "Env/Length/Episode (New)                             |  187.91\n",
      "Env/Reward/Episode (Last 50)                         |  188.78\n",
      "Env/Length/Episode (Last 50)                         |  188.78\n",
      "policy/loss                                          |   21.99\n",
      "v/loss                                               | 2060.74\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  189.20\n",
      "Env/Length/Episode (New)                             |  189.20\n",
      "Env/Reward/Episode (Last 50)                         |  189.72\n",
      "Env/Length/Episode (Last 50)                         |  189.72\n",
      "policy/loss                                          |   21.74\n",
      "v/loss                                               | 2014.55\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  188.75\n",
      "Env/Length/Episode (New)                             |  188.75\n",
      "Env/Reward/Episode (Last 50)                         |  192.12\n",
      "Env/Length/Episode (Last 50)                         |  192.12\n",
      "policy/loss                                          |   17.56\n",
      "v/loss                                               | 1431.57\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  190.74\n",
      "Env/Length/Episode (New)                             |  190.74\n",
      "Env/Reward/Episode (Last 50)                         |  190.18\n",
      "Env/Length/Episode (Last 50)                         |  190.18\n",
      "policy/loss                                          |   20.02\n",
      "v/loss                                               | 1861.77\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  193.57\n",
      "Env/Length/Episode (New)                             |  193.57\n",
      "Env/Reward/Episode (Last 50)                         |  193.18\n",
      "Env/Length/Episode (Last 50)                         |  193.18\n",
      "policy/loss                                          |   19.68\n",
      "v/loss                                               | 1688.45\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  187.67\n",
      "Env/Length/Episode (New)                             |  187.67\n",
      "Env/Reward/Episode (Last 50)                         |  186.44\n",
      "Env/Length/Episode (Last 50)                         |  186.44\n",
      "policy/loss                                          |   15.95\n",
      "v/loss                                               | 1486.54\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  185.31\n",
      "Env/Length/Episode (New)                             |  185.31\n",
      "Env/Reward/Episode (Last 50)                         |  185.88\n",
      "Env/Length/Episode (Last 50)                         |  185.88\n",
      "policy/loss                                          |   17.66\n",
      "v/loss                                               | 1500.54\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  193.75\n",
      "Env/Length/Episode (New)                             |  193.75\n",
      "Env/Reward/Episode (Last 50)                         |  195.76\n",
      "Env/Length/Episode (Last 50)                         |  195.76\n",
      "policy/loss                                          |   16.68\n",
      "v/loss                                               | 1488.42\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  192.85\n",
      "Env/Length/Episode (New)                             |  192.85\n",
      "Env/Reward/Episode (Last 50)                         |  192.42\n",
      "Env/Length/Episode (Last 50)                         |  192.42\n",
      "policy/loss                                          |   13.33\n",
      "v/loss                                               | 1110.07\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  187.95\n",
      "Env/Length/Episode (New)                             |  187.95\n",
      "Env/Reward/Episode (Last 50)                         |  188.84\n",
      "Env/Length/Episode (Last 50)                         |  188.84\n",
      "policy/loss                                          |   12.76\n",
      "v/loss                                               | 1150.06\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  189.91\n",
      "Env/Length/Episode (New)                             |  189.91\n",
      "Env/Reward/Episode (Last 50)                         |  189.70\n",
      "Env/Length/Episode (Last 50)                         |  189.70\n",
      "policy/loss                                          |   12.33\n",
      "v/loss                                               | 1123.45\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  196.85\n",
      "Env/Length/Episode (New)                             |  196.85\n",
      "Env/Reward/Episode (Last 50)                         |  196.78\n",
      "Env/Length/Episode (Last 50)                         |  196.78\n",
      "policy/loss                                          |   12.58\n",
      "v/loss                                               | 1039.69\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                              | 199.06\n",
      "Env/Length/Episode (New)                              | 199.06\n",
      "Env/Reward/Episode (Last 50)                          | 199.04\n",
      "Env/Length/Episode (Last 50)                          | 199.04\n",
      "policy/loss                                           |  11.82\n",
      "v/loss                                                | 923.45\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                             |  196.31\n",
      "Env/Length/Episode (New)                             |  196.31\n",
      "Env/Reward/Episode (Last 50)                         |  196.16\n",
      "Env/Length/Episode (Last 50)                         |  196.16\n",
      "policy/loss                                          |   14.00\n",
      "v/loss                                               | 1104.11\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                              | 189.85\n",
      "Env/Length/Episode (New)                              | 189.85\n",
      "Env/Reward/Episode (Last 50)                          | 189.04\n",
      "Env/Length/Episode (Last 50)                          | 189.04\n",
      "policy/loss                                           |  11.67\n",
      "v/loss                                                | 893.57\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in batcher.get_batches(max_steps=3e5, get_action_fn=policy.get_action):\n",
    "    batch = batch.to_tensor()\n",
    "    # Calculate state value\n",
    "    state_t = U.join_first_dims(batch.state_t, 2)\n",
    "    v_t = v_nn(state_t)\n",
    "    # Calculate return\n",
    "    ret = U.estimators.discounted_sum_rewards(\n",
    "        rewards=batch.reward,\n",
    "        dones=batch.done,\n",
    "        gamma=GAMMA,\n",
    "        v_t_last=v_t[-1]\n",
    "    ).detach()\n",
    "    batch = batch.concat_batch()\n",
    "    # Calculate advantage    \n",
    "    adv = (ret - v_t).detach()\n",
    "    \n",
    "    # Calculate policy loss\n",
    "    dist = policy.create_dist(batch.state_t)\n",
    "    log_prob = dist.log_prob(batch.action)\n",
    "    assert ret.shape == log_prob.shape\n",
    "    p_loss = -(adv * log_prob).mean()\n",
    "    \n",
    "    # Calculate value loss\n",
    "    v_loss = F.mse_loss(v_t, ret)\n",
    "    \n",
    "    # Optimize\n",
    "    p_opt.zero_grad()\n",
    "    p_loss.backward()\n",
    "    p_opt.step()\n",
    "    \n",
    "    v_opt.zero_grad()\n",
    "    v_loss.backward()\n",
    "    v_opt.step()\n",
    "    \n",
    "    # Write logs    \n",
    "    if batcher.num_steps > last_logged_step:\n",
    "        last_logged_step = batcher.num_steps + 10000\n",
    "        batcher.write_logs(logger)\n",
    "        logger.add_log('policy/loss', p_loss)\n",
    "        logger.add_log('v/loss', v_loss)\n",
    "        logger.log(step=batcher.num_steps)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
