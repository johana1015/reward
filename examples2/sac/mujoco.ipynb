{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/envs/torchrl_up/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import reward as rw\n",
    "import reward.utils as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'Humanoid-v2'\n",
    "LOG_DIR = 'logs/humanoid/paper-v0-1'\n",
    "REPAR = True\n",
    "REWARD_SCALE = 20.\n",
    "CLIP_GRAD = float('inf')\n",
    "GAMMA = 0.99\n",
    "TARGET_UP_WEIGHT = 0.005\n",
    "BATCH_SIZE = 256\n",
    "MAX_STEPS = 40e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the latest nvidia driver: /usr/lib/nvidia-390, among ['/usr/lib/nvidia-375', '/usr/lib/nvidia-390']\n",
      "Choosing the latest nvidia driver: /usr/lib/nvidia-390, among ['/usr/lib/nvidia-375', '/usr/lib/nvidia-390']\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = rw.envs.GymEnv(ENV)\n",
    "env = rw.envs.wrappers.ActionBound(env)\n",
    "runner = rw.runners.SingleRunner(env)\n",
    "batcher = rw.batchers.ReplayBatcher(\n",
    "    runner=runner,\n",
    "    batch_size=128,\n",
    "    replay_buffer_maxlen=1e6,\n",
    "    learning_freq=1,\n",
    "    grad_steps_per_batch=1,\n",
    "    transforms=[\n",
    "#         rw.batchers.transforms.StateRunNorm(),\n",
    "        rw.batchers.transforms.RewardConstScaler(REWARD_SCALE),\n",
    "    ],\n",
    ")\n",
    "\n",
    "state_features = batcher.get_state_info().shape[0]\n",
    "num_actions = batcher.get_action_info().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_units=256,\n",
    "                 activation=nn.ReLU, log_std_range=(-20, 2)):\n",
    "        super().__init__()\n",
    "        self.log_std_range = log_std_range\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(num_inputs, hidden_units), activation()]\n",
    "        layers += [nn.Linear(hidden_units, hidden_units), activation()]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_units, num_outputs)\n",
    "        self.mean.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.mean.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "        self.log_std = nn.Linear(hidden_units, num_outputs)\n",
    "        self.log_std.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.log_std.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x).clamp(*self.log_std_range)\n",
    "        return mean, log_std        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNN(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_units=256, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(num_inputs, hidden_units), activation()]\n",
    "        layers += [nn.Linear(hidden_units, hidden_units), activation()]\n",
    "        final_layer = nn.Linear(hidden_units, 1)\n",
    "        final_layer.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        final_layer.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        layers += [final_layer]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_units=256, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(num_inputs + num_actions, hidden_units), activation()]\n",
    "        layers += [nn.Linear(hidden_units, hidden_units), activation()]\n",
    "        final_layer = nn.Linear(hidden_units, 1)\n",
    "        final_layer.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        final_layer.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        layers += [final_layer]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        state, action = x\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNormalPolicy(rw.policy.BasePolicy):\n",
    "    def create_dist(self, state):\n",
    "        parameters = self.nn(state)\n",
    "        mean, log_std = parameters\n",
    "        return rw.distributions.TanhNormal(loc=mean, scale=log_std.exp())\n",
    "\n",
    "    def get_action(self, state, step):\n",
    "        dist = self.create_dist(state=state)\n",
    "        action = U.to_np(dist.sample())\n",
    "        assert not np.isnan(action).any()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nn = PolicyNN(num_inputs=state_features, num_outputs=num_actions).to(device)\n",
    "v_nn = ValueNN(num_inputs=state_features).to(device)\n",
    "v_nn_target = ValueNN(num_inputs=state_features).to(device).eval()\n",
    "q1_nn = QValueNN(num_inputs=state_features, num_actions=num_actions).to(device)\n",
    "q2_nn = QValueNN(num_inputs=state_features, num_actions=num_actions).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.copy_weights(from_nn=v_nn, to_nn=v_nn_target, weight=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TanhNormalPolicy(nn=p_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_opt = torch.optim.Adam(p_nn.parameters(), lr=3e-4)\n",
    "v_opt = torch.optim.Adam(v_nn.parameters(), lr=3e-4)\n",
    "q1_opt = torch.optim.Adam(q1_nn.parameters(), lr=3e-4)\n",
    "q2_opt = torch.optim.Adam(q2_nn.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to: logs/humanoid/paper-v0-1\n"
     ]
    }
   ],
   "source": [
    "logger = U.Logger(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Replay Buffer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c55b478600043a2a8c571ad6e5716f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batcher.populate(n=1000, get_action_fn=policy.get_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f300249588234925a874f437d1db14f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=40000000), HTML(value='')), layout=Layout(disâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   121.18\n",
      "Env/Length/Episode (New)                            |    24.49\n",
      "Env/Reward/Episode (Last 50)                        |   178.31\n",
      "Env/Length/Episode (Last 50)                        |    34.74\n",
      "policy/loss                                         | -1071.56\n",
      "v/loss                                              |  3609.77\n",
      "q1/loss                                             |  8477.33\n",
      "q2/loss                                             |  8528.21\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  -375.73\n",
      "q1/grad                                             |   160.64\n",
      "q2/grad                                             |    48.89\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   279.36\n",
      "Env/Length/Episode (New)                            |    55.79\n",
      "Env/Reward/Episode (Last 50)                        |   303.58\n",
      "Env/Length/Episode (Last 50)                        |    61.20\n",
      "policy/loss                                         | -2002.04\n",
      "v/loss                                              |  5787.38\n",
      "q1/loss                                             |  8966.82\n",
      "q2/loss                                             |  8835.03\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  -314.71\n",
      "q1/grad                                             |    71.38\n",
      "q2/grad                                             |    93.02\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   355.26\n",
      "Env/Length/Episode (New)                            |    71.46\n",
      "Env/Reward/Episode (Last 50)                        |   350.60\n",
      "Env/Length/Episode (Last 50)                        |    70.48\n",
      "policy/loss                                         | -2265.18\n",
      "v/loss                                              |  5618.13\n",
      "q1/loss                                             |  8083.38\n",
      "q2/loss                                             |  7974.85\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  -351.52\n",
      "q1/grad                                             |   300.77\n",
      "q2/grad                                             |   547.75\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   315.33\n",
      "Env/Length/Episode (New)                            |    62.33\n",
      "Env/Reward/Episode (Last 50)                        |   310.32\n",
      "Env/Length/Episode (Last 50)                        |    61.04\n",
      "policy/loss                                         | -2190.36\n",
      "v/loss                                              |  8362.88\n",
      "q1/loss                                             |  8891.82\n",
      "q2/loss                                             |  8897.64\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              | -1070.66\n",
      "q1/grad                                             |   628.12\n",
      "q2/grad                                             |   574.53\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   347.90\n",
      "Env/Length/Episode (New)                            |    67.07\n",
      "Env/Reward/Episode (Last 50)                        |   357.39\n",
      "Env/Length/Episode (Last 50)                        |    69.06\n",
      "policy/loss                                         | -2314.46\n",
      "v/loss                                              |  5572.42\n",
      "q1/loss                                             |  8755.36\n",
      "q2/loss                                             |  9412.68\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  -275.77\n",
      "q1/grad                                             |   239.94\n",
      "q2/grad                                             |  -152.31\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   363.02\n",
      "Env/Length/Episode (New)                            |    72.00\n",
      "Env/Reward/Episode (Last 50)                        |   363.60\n",
      "Env/Length/Episode (Last 50)                        |    71.76\n",
      "policy/loss                                         | -2356.32\n",
      "v/loss                                              |  5244.74\n",
      "q1/loss                                             | 12991.63\n",
      "q2/loss                                             | 10544.31\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |   833.48\n",
      "q1/grad                                             | -1108.22\n",
      "q2/grad                                             |  -403.08\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   407.03\n",
      "Env/Length/Episode (New)                            |    85.26\n",
      "Env/Reward/Episode (Last 50)                        |   399.12\n",
      "Env/Length/Episode (Last 50)                        |    83.82\n",
      "policy/loss                                         | -2310.33\n",
      "v/loss                                              |  4469.61\n",
      "q1/loss                                             |  8102.62\n",
      "q2/loss                                             |  9272.14\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |   375.38\n",
      "q1/grad                                             |   115.88\n",
      "q2/grad                                             |  -236.69\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   408.09\n",
      "Env/Length/Episode (New)                            |    86.70\n",
      "Env/Reward/Episode (Last 50)                        |   420.27\n",
      "Env/Length/Episode (Last 50)                        |    88.82\n",
      "policy/loss                                         | -2225.62\n",
      "v/loss                                              |  6335.99\n",
      "q1/loss                                             | 11924.82\n",
      "q2/loss                                             | 13533.36\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |   182.29\n",
      "q1/grad                                             |   214.85\n",
      "q2/grad                                             |   113.68\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   390.88\n",
      "Env/Length/Episode (New)                            |    83.88\n",
      "Env/Reward/Episode (Last 50)                        |   392.73\n",
      "Env/Length/Episode (Last 50)                        |    84.22\n",
      "policy/loss                                         | -2256.11\n",
      "v/loss                                              |  4678.13\n",
      "q1/loss                                             | 11966.65\n",
      "q2/loss                                             |  9266.34\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |  -159.95\n",
      "q1/grad                                             |  -287.22\n",
      "q2/grad                                             |   -65.19\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   400.04\n",
      "Env/Length/Episode (New)                            |    85.20\n",
      "Env/Reward/Episode (Last 50)                        |   402.99\n",
      "Env/Length/Episode (Last 50)                        |    85.86\n",
      "policy/loss                                         | -2430.41\n",
      "v/loss                                              |  4289.85\n",
      "q1/loss                                             |  6594.27\n",
      "q2/loss                                             |  5743.15\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |  -972.29\n",
      "q1/grad                                             |   159.44\n",
      "q2/grad                                             |   112.75\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   379.81\n",
      "Env/Length/Episode (New)                            |    78.71\n",
      "Env/Reward/Episode (Last 50)                        |   376.54\n",
      "Env/Length/Episode (Last 50)                        |    78.08\n",
      "policy/loss                                         | -2135.46\n",
      "v/loss                                              |  7274.07\n",
      "q1/loss                                             |  7576.76\n",
      "q2/loss                                             |  9372.70\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  1775.26\n",
      "q1/grad                                             |    97.51\n",
      "q2/grad                                             |  -486.52\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   380.35\n",
      "Env/Length/Episode (New)                            |    78.27\n",
      "Env/Reward/Episode (Last 50)                        |   377.51\n",
      "Env/Length/Episode (Last 50)                        |    77.72\n",
      "policy/loss                                         | -2658.83\n",
      "v/loss                                              |  5334.96\n",
      "q1/loss                                             |  6409.03\n",
      "q2/loss                                             |  6877.86\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |  -960.61\n",
      "q1/grad                                             |   624.23\n",
      "q2/grad                                             |   273.47\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   383.89\n",
      "Env/Length/Episode (New)                            |    78.41\n",
      "Env/Reward/Episode (Last 50)                        |   382.94\n",
      "Env/Length/Episode (Last 50)                        |    78.32\n",
      "policy/loss                                         | -2553.69\n",
      "v/loss                                              |  4441.36\n",
      "q1/loss                                             |  7944.03\n",
      "q2/loss                                             |  7276.22\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              | -1048.97\n",
      "q1/grad                                             |   588.17\n",
      "q2/grad                                             |    62.96\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   438.01\n",
      "Env/Length/Episode (New)                            |    86.46\n",
      "Env/Reward/Episode (Last 50)                        |   428.52\n",
      "Env/Length/Episode (Last 50)                        |    84.42\n",
      "policy/loss                                         | -2529.63\n",
      "v/loss                                              |  5622.25\n",
      "q1/loss                                             |  7690.96\n",
      "q2/loss                                             |  8602.20\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |   139.76\n",
      "q1/grad                                             |    78.46\n",
      "q2/grad                                             |   -61.06\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   382.95\n",
      "Env/Length/Episode (New)                            |    76.00\n",
      "Env/Reward/Episode (Last 50)                        |   381.26\n",
      "Env/Length/Episode (Last 50)                        |    75.76\n",
      "policy/loss                                         | -2695.30\n",
      "v/loss                                              |  5233.73\n",
      "q1/loss                                             |  8976.43\n",
      "q2/loss                                             |  7883.49\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  1727.32\n",
      "q1/grad                                             |  -695.72\n",
      "q2/grad                                             |  -446.35\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   430.07\n",
      "Env/Length/Episode (New)                            |    85.19\n",
      "Env/Reward/Episode (Last 50)                        |   429.63\n",
      "Env/Length/Episode (Last 50)                        |    84.98\n",
      "policy/loss                                         | -2659.62\n",
      "v/loss                                              |  6720.72\n",
      "q1/loss                                             | 12776.53\n",
      "q2/loss                                             | 15163.16\n",
      "policy/grad                                         |     0.03\n",
      "v/grad                                              |  1222.94\n",
      "q1/grad                                             |   180.23\n",
      "q2/grad                                             |  -166.60\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   403.23\n",
      "Env/Length/Episode (New)                            |    81.22\n",
      "Env/Reward/Episode (Last 50)                        |   404.16\n",
      "Env/Length/Episode (Last 50)                        |    81.36\n",
      "policy/loss                                         | -2694.06\n",
      "v/loss                                              |  5784.72\n",
      "q1/loss                                             |  6635.88\n",
      "q2/loss                                             |  5045.70\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |   657.13\n",
      "q1/grad                                             |   129.48\n",
      "q2/grad                                             |    60.06\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   431.06\n",
      "Env/Length/Episode (New)                            |    86.83\n",
      "Env/Reward/Episode (Last 50)                        |   431.98\n",
      "Env/Length/Episode (Last 50)                        |    86.92\n",
      "policy/loss                                         | -2785.61\n",
      "v/loss                                              |  3688.23\n",
      "q1/loss                                             |  8674.91\n",
      "q2/loss                                             |  7058.03\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |  -269.65\n",
      "q1/grad                                             |   188.37\n",
      "q2/grad                                             |   352.10\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   447.04\n",
      "Env/Length/Episode (New)                            |    92.42\n",
      "Env/Reward/Episode (Last 50)                        |   444.85\n",
      "Env/Length/Episode (Last 50)                        |    92.02\n",
      "policy/loss                                         | -2752.29\n",
      "v/loss                                              |  6169.22\n",
      "q1/loss                                             |  6872.67\n",
      "q2/loss                                             |  7028.52\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  1172.95\n",
      "q1/grad                                             |   -48.78\n",
      "q2/grad                                             |  -180.06\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   460.51\n",
      "Env/Length/Episode (New)                            |    93.88\n",
      "Env/Reward/Episode (Last 50)                        |   461.62\n",
      "Env/Length/Episode (Last 50)                        |    95.00\n",
      "policy/loss                                         | -2875.79\n",
      "v/loss                                              |  3292.89\n",
      "q1/loss                                             |  7100.06\n",
      "q2/loss                                             |  6984.31\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |   166.30\n",
      "q1/grad                                             |    52.92\n",
      "q2/grad                                             |  -572.99\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   437.22\n",
      "Env/Length/Episode (New)                            |    88.09\n",
      "Env/Reward/Episode (Last 50)                        |   440.59\n",
      "Env/Length/Episode (Last 50)                        |    88.56\n",
      "policy/loss                                         | -2538.03\n",
      "v/loss                                              |  3858.49\n",
      "q1/loss                                             | 12747.73\n",
      "q2/loss                                             | 13180.04\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  -270.66\n",
      "q1/grad                                             |  -389.09\n",
      "q2/grad                                             |  -182.70\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   446.70\n",
      "Env/Length/Episode (New)                            |    93.28\n",
      "Env/Reward/Episode (Last 50)                        |   451.73\n",
      "Env/Length/Episode (Last 50)                        |    93.54\n",
      "policy/loss                                         | -2661.43\n",
      "v/loss                                              |  6050.23\n",
      "q1/loss                                             |  8694.98\n",
      "q2/loss                                             |  8010.48\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |   936.84\n",
      "q1/grad                                             |  -720.96\n",
      "q2/grad                                             |  -176.23\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   425.28\n",
      "Env/Length/Episode (New)                            |    87.98\n",
      "Env/Reward/Episode (Last 50)                        |   430.25\n",
      "Env/Length/Episode (Last 50)                        |    88.62\n",
      "policy/loss                                         | -2811.61\n",
      "v/loss                                              |  3753.57\n",
      "q1/loss                                             |  9369.03\n",
      "q2/loss                                             |  8243.08\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |  -654.88\n",
      "q1/grad                                             |   315.23\n",
      "q2/grad                                             |   480.65\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   457.50\n",
      "Env/Length/Episode (New)                            |    93.47\n",
      "Env/Reward/Episode (Last 50)                        |   453.87\n",
      "Env/Length/Episode (Last 50)                        |    93.08\n",
      "policy/loss                                         | -2655.19\n",
      "v/loss                                              |  5933.51\n",
      "q1/loss                                             | 12514.70\n",
      "q2/loss                                             | 10774.15\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |    30.51\n",
      "q1/grad                                             |   436.96\n",
      "q2/grad                                             |   209.81\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   429.61\n",
      "Env/Length/Episode (New)                            |    85.23\n",
      "Env/Reward/Episode (Last 50)                        |   428.08\n",
      "Env/Length/Episode (Last 50)                        |    84.82\n",
      "policy/loss                                         | -2645.50\n",
      "v/loss                                              |  3272.94\n",
      "q1/loss                                             |  6875.98\n",
      "q2/loss                                             | 10068.11\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  -277.16\n",
      "q1/grad                                             |   347.14\n",
      "q2/grad                                             |   493.54\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   462.54\n",
      "Env/Length/Episode (New)                            |    92.74\n",
      "Env/Reward/Episode (Last 50)                        |   443.11\n",
      "Env/Length/Episode (Last 50)                        |    88.50\n",
      "policy/loss                                         | -2849.10\n",
      "v/loss                                              |  3914.16\n",
      "q1/loss                                             |  9155.77\n",
      "q2/loss                                             | 10271.79\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |   -42.60\n",
      "q1/grad                                             |  -608.44\n",
      "q2/grad                                             |  -455.87\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   523.96\n",
      "Env/Length/Episode (New)                            |   103.33\n",
      "Env/Reward/Episode (Last 50)                        |   511.92\n",
      "Env/Length/Episode (Last 50)                        |   101.40\n",
      "policy/loss                                         | -2863.84\n",
      "v/loss                                              |  6999.88\n",
      "q1/loss                                             | 13373.49\n",
      "q2/loss                                             | 12281.49\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |  -733.76\n",
      "q1/grad                                             |  -155.39\n",
      "q2/grad                                             |    99.55\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   519.15\n",
      "Env/Length/Episode (New)                            |   103.49\n",
      "Env/Reward/Episode (Last 50)                        |   523.45\n",
      "Env/Length/Episode (Last 50)                        |   104.36\n",
      "policy/loss                                         | -2935.81\n",
      "v/loss                                              |  2257.67\n",
      "q1/loss                                             |  6193.84\n",
      "q2/loss                                             |  5637.24\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  -367.41\n",
      "q1/grad                                             |   261.17\n",
      "q2/grad                                             |   201.56\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   513.02\n",
      "Env/Length/Episode (New)                            |   101.46\n",
      "Env/Reward/Episode (Last 50)                        |   522.66\n",
      "Env/Length/Episode (Last 50)                        |   104.38\n",
      "policy/loss                                         | -2811.44\n",
      "v/loss                                              |  5326.73\n",
      "q1/loss                                             | 12700.01\n",
      "q2/loss                                             | 19068.41\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  1618.51\n",
      "q1/grad                                             |   -21.68\n",
      "q2/grad                                             |    69.25\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   523.25\n",
      "Env/Length/Episode (New)                            |   103.31\n",
      "Env/Reward/Episode (Last 50)                        |   523.71\n",
      "Env/Length/Episode (Last 50)                        |   103.12\n",
      "policy/loss                                         | -2865.59\n",
      "v/loss                                              |  4893.25\n",
      "q1/loss                                             |  9932.00\n",
      "q2/loss                                             | 11478.93\n",
      "policy/grad                                         |    -0.03\n",
      "v/grad                                              |  -832.84\n",
      "q1/grad                                             |   464.26\n",
      "q2/grad                                             |   193.57\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   547.01\n",
      "Env/Length/Episode (New)                            |   110.11\n",
      "Env/Reward/Episode (Last 50)                        |   530.44\n",
      "Env/Length/Episode (Last 50)                        |   106.32\n",
      "policy/loss                                         | -3025.18\n",
      "v/loss                                              |  3673.85\n",
      "q1/loss                                             |  7223.64\n",
      "q2/loss                                             |  7452.43\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |   261.32\n",
      "q1/grad                                             |   748.79\n",
      "q2/grad                                             |   395.08\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   535.54\n",
      "Env/Length/Episode (New)                            |   108.41\n",
      "Env/Reward/Episode (Last 50)                        |   538.25\n",
      "Env/Length/Episode (Last 50)                        |   109.16\n",
      "policy/loss                                         | -3139.16\n",
      "v/loss                                              |  5912.35\n",
      "q1/loss                                             |  7825.25\n",
      "q2/loss                                             | 10460.85\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  1359.69\n",
      "q1/grad                                             |  -191.92\n",
      "q2/grad                                             |  -763.44\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   521.49\n",
      "Env/Length/Episode (New)                            |   105.14\n",
      "Env/Reward/Episode (Last 50)                        |   518.40\n",
      "Env/Length/Episode (Last 50)                        |   104.84\n",
      "policy/loss                                         | -3007.42\n",
      "v/loss                                              |  4807.08\n",
      "q1/loss                                             |  9492.69\n",
      "q2/loss                                             |  8714.14\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |   798.38\n",
      "q1/grad                                             |  -707.60\n",
      "q2/grad                                             |  -372.73\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   490.79\n",
      "Env/Length/Episode (New)                            |    97.90\n",
      "Env/Reward/Episode (Last 50)                        |   500.84\n",
      "Env/Length/Episode (Last 50)                        |    99.96\n",
      "policy/loss                                         | -3005.13\n",
      "v/loss                                              |  7712.83\n",
      "q1/loss                                             | 11275.71\n",
      "q2/loss                                             | 11233.28\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              | -1970.77\n",
      "q1/grad                                             |  1035.86\n",
      "q2/grad                                             |  1017.67\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   517.03\n",
      "Env/Length/Episode (New)                            |   103.74\n",
      "Env/Reward/Episode (Last 50)                        |   505.47\n",
      "Env/Length/Episode (Last 50)                        |   100.70\n",
      "policy/loss                                         | -2709.98\n",
      "v/loss                                              |  4829.17\n",
      "q1/loss                                             |  7194.22\n",
      "q2/loss                                             |  7645.79\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              | -1549.18\n",
      "q1/grad                                             |   684.48\n",
      "q2/grad                                             |   433.55\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   516.21\n",
      "Env/Length/Episode (New)                            |   102.62\n",
      "Env/Reward/Episode (Last 50)                        |   519.14\n",
      "Env/Length/Episode (Last 50)                        |   103.48\n",
      "policy/loss                                         | -2976.62\n",
      "v/loss                                              |  7265.15\n",
      "q1/loss                                             |  7059.42\n",
      "q2/loss                                             |  8635.59\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  2397.97\n",
      "q1/grad                                             |  -223.38\n",
      "q2/grad                                             |   -66.34\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   523.12\n",
      "Env/Length/Episode (New)                            |   104.41\n",
      "Env/Reward/Episode (Last 50)                        |   529.63\n",
      "Env/Length/Episode (Last 50)                        |   105.28\n",
      "policy/loss                                         | -3095.78\n",
      "v/loss                                              |  5872.60\n",
      "q1/loss                                             |  8300.88\n",
      "q2/loss                                             |  6724.64\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |   183.53\n",
      "q1/grad                                             |   204.26\n",
      "q2/grad                                             |    99.84\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   534.15\n",
      "Env/Length/Episode (New)                            |   104.42\n",
      "Env/Reward/Episode (Last 50)                        |   533.24\n",
      "Env/Length/Episode (Last 50)                        |   104.32\n",
      "policy/loss                                         | -3158.94\n",
      "v/loss                                              | 16238.53\n",
      "q1/loss                                             | 10818.05\n",
      "q2/loss                                             | 11712.13\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              | -4563.06\n",
      "q1/grad                                             |   816.19\n",
      "q2/grad                                             |   639.69\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   537.36\n",
      "Env/Length/Episode (New)                            |   109.57\n",
      "Env/Reward/Episode (Last 50)                        |   534.02\n",
      "Env/Length/Episode (Last 50)                        |   108.24\n",
      "policy/loss                                         | -3076.79\n",
      "v/loss                                              |  4396.99\n",
      "q1/loss                                             | 12035.62\n",
      "q2/loss                                             |  8773.22\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |  -851.23\n",
      "q1/grad                                             |  1005.97\n",
      "q2/grad                                             |   621.14\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   523.00\n",
      "Env/Length/Episode (New)                            |   105.57\n",
      "Env/Reward/Episode (Last 50)                        |   534.40\n",
      "Env/Length/Episode (Last 50)                        |   108.62\n",
      "policy/loss                                         | -2992.44\n",
      "v/loss                                              |  3712.86\n",
      "q1/loss                                             | 11456.52\n",
      "q2/loss                                             | 11003.37\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |   985.25\n",
      "q1/grad                                             |  -417.13\n",
      "q2/grad                                             |   -60.34\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   472.94\n",
      "Env/Length/Episode (New)                            |    98.44\n",
      "Env/Reward/Episode (Last 50)                        |   477.25\n",
      "Env/Length/Episode (Last 50)                        |    98.32\n",
      "policy/loss                                         | -3178.41\n",
      "v/loss                                              |  4266.91\n",
      "q1/loss                                             |  6185.70\n",
      "q2/loss                                             |  6122.18\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |   921.86\n",
      "q1/grad                                             |   -67.88\n",
      "q2/grad                                             |   -75.42\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   506.96\n",
      "Env/Length/Episode (New)                            |   101.33\n",
      "Env/Reward/Episode (Last 50)                        |   494.06\n",
      "Env/Length/Episode (Last 50)                        |    99.24\n",
      "policy/loss                                         | -2885.63\n",
      "v/loss                                              |  6081.33\n",
      "q1/loss                                             |  9381.99\n",
      "q2/loss                                             |  8723.54\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  -558.30\n",
      "q1/grad                                             |  -121.90\n",
      "q2/grad                                             |   -15.17\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   523.38\n",
      "Env/Length/Episode (New)                            |   105.73\n",
      "Env/Reward/Episode (Last 50)                        |   518.56\n",
      "Env/Length/Episode (Last 50)                        |   104.10\n",
      "policy/loss                                         | -3248.75\n",
      "v/loss                                              |  5134.11\n",
      "q1/loss                                             |  9995.45\n",
      "q2/loss                                             |  8572.87\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |  -653.53\n",
      "q1/grad                                             |   -83.90\n",
      "q2/grad                                             |   179.30\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   524.16\n",
      "Env/Length/Episode (New)                            |   103.28\n",
      "Env/Reward/Episode (Last 50)                        |   518.91\n",
      "Env/Length/Episode (Last 50)                        |   103.52\n",
      "policy/loss                                         | -3222.58\n",
      "v/loss                                              |  3422.84\n",
      "q1/loss                                             |  4812.21\n",
      "q2/loss                                             |  5750.64\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |   274.53\n",
      "q1/grad                                             |  -111.41\n",
      "q2/grad                                             |   -18.38\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   581.34\n",
      "Env/Length/Episode (New)                            |   115.00\n",
      "Env/Reward/Episode (Last 50)                        |   564.20\n",
      "Env/Length/Episode (Last 50)                        |   111.52\n",
      "policy/loss                                         | -3006.03\n",
      "v/loss                                              |  4772.60\n",
      "q1/loss                                             |  7119.44\n",
      "q2/loss                                             |  7940.81\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  -425.22\n",
      "q1/grad                                             |   350.48\n",
      "q2/grad                                             |   588.29\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   513.96\n",
      "Env/Length/Episode (New)                            |    99.28\n",
      "Env/Reward/Episode (Last 50)                        |   529.17\n",
      "Env/Length/Episode (Last 50)                        |   102.42\n",
      "policy/loss                                         | -2904.88\n",
      "v/loss                                              |  3381.76\n",
      "q1/loss                                             |  6170.63\n",
      "q2/loss                                             |  7583.63\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  -691.49\n",
      "q1/grad                                             |    83.58\n",
      "q2/grad                                             |   195.63\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   632.43\n",
      "Env/Length/Episode (New)                            |   126.06\n",
      "Env/Reward/Episode (Last 50)                        |   588.36\n",
      "Env/Length/Episode (Last 50)                        |   115.96\n",
      "policy/loss                                         | -2930.80\n",
      "v/loss                                              |  5595.83\n",
      "q1/loss                                             | 14548.97\n",
      "q2/loss                                             | 11755.62\n",
      "policy/grad                                         |    -0.00\n",
      "v/grad                                              |  1726.76\n",
      "q1/grad                                             |  -497.51\n",
      "q2/grad                                             |  -392.53\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   600.17\n",
      "Env/Length/Episode (New)                            |   120.91\n",
      "Env/Reward/Episode (Last 50)                        |   594.29\n",
      "Env/Length/Episode (Last 50)                        |   119.46\n",
      "policy/loss                                         | -3218.85\n",
      "v/loss                                              |  3347.43\n",
      "q1/loss                                             | 10436.86\n",
      "q2/loss                                             | 10422.02\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |  1020.53\n",
      "q1/grad                                             |   309.96\n",
      "q2/grad                                             |  -314.85\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   629.09\n",
      "Env/Length/Episode (New)                            |   126.09\n",
      "Env/Reward/Episode (Last 50)                        |   636.81\n",
      "Env/Length/Episode (Last 50)                        |   128.70\n",
      "policy/loss                                         | -3403.02\n",
      "v/loss                                              |  7088.55\n",
      "q1/loss                                             | 10753.27\n",
      "q2/loss                                             |  9075.12\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |  1472.18\n",
      "q1/grad                                             |  -285.87\n",
      "q2/grad                                             |   -91.68\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   619.76\n",
      "Env/Length/Episode (New)                            |   124.72\n",
      "Env/Reward/Episode (Last 50)                        |   613.57\n",
      "Env/Length/Episode (Last 50)                        |   123.46\n",
      "policy/loss                                         | -3179.41\n",
      "v/loss                                              |  4007.29\n",
      "q1/loss                                             | 11596.78\n",
      "q2/loss                                             |  9442.36\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  -608.82\n",
      "q1/grad                                             |  -185.64\n",
      "q2/grad                                             |    53.06\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   644.77\n",
      "Env/Length/Episode (New)                            |   131.70\n",
      "Env/Reward/Episode (Last 50)                        |   654.50\n",
      "Env/Length/Episode (Last 50)                        |   132.88\n",
      "policy/loss                                         | -3017.27\n",
      "v/loss                                              |  5098.58\n",
      "q1/loss                                             | 31413.42\n",
      "q2/loss                                             | 32794.40\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |   642.67\n",
      "q1/grad                                             |   703.30\n",
      "q2/grad                                             |   441.52\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   674.60\n",
      "Env/Length/Episode (New)                            |   135.30\n",
      "Env/Reward/Episode (Last 50)                        |   650.70\n",
      "Env/Length/Episode (Last 50)                        |   131.44\n",
      "policy/loss                                         | -3411.21\n",
      "v/loss                                              |  4148.53\n",
      "q1/loss                                             | 11320.64\n",
      "q2/loss                                             | 12152.93\n",
      "policy/grad                                         |    -0.03\n",
      "v/grad                                              | -1159.89\n",
      "q1/grad                                             |    71.54\n",
      "q2/grad                                             |   490.60\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   597.58\n",
      "Env/Length/Episode (New)                            |   120.06\n",
      "Env/Reward/Episode (Last 50)                        |   609.44\n",
      "Env/Length/Episode (Last 50)                        |   122.42\n",
      "policy/loss                                         | -3456.51\n",
      "v/loss                                              |  3208.70\n",
      "q1/loss                                             | 12152.91\n",
      "q2/loss                                             | 11838.13\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |   995.18\n",
      "q1/grad                                             |  -371.43\n",
      "q2/grad                                             |  -449.46\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   707.89\n",
      "Env/Length/Episode (New)                            |   142.29\n",
      "Env/Reward/Episode (Last 50)                        |   653.29\n",
      "Env/Length/Episode (Last 50)                        |   131.86\n",
      "policy/loss                                         | -3152.79\n",
      "v/loss                                              |  5083.40\n",
      "q1/loss                                             |  8603.66\n",
      "q2/loss                                             | 10049.36\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |   912.94\n",
      "q1/grad                                             |  -181.92\n",
      "q2/grad                                             |  -389.05\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   569.10\n",
      "Env/Length/Episode (New)                            |   112.97\n",
      "Env/Reward/Episode (Last 50)                        |   627.05\n",
      "Env/Length/Episode (Last 50)                        |   124.66\n",
      "policy/loss                                         | -3335.47\n",
      "v/loss                                              |  4041.72\n",
      "q1/loss                                             |  8505.07\n",
      "q2/loss                                             |  9432.77\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              |  -238.29\n",
      "q1/grad                                             |  -260.09\n",
      "q2/grad                                             |   -42.17\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   726.75\n",
      "Env/Length/Episode (New)                            |   143.00\n",
      "Env/Reward/Episode (Last 50)                        |   647.25\n",
      "Env/Length/Episode (Last 50)                        |   127.58\n",
      "policy/loss                                         | -3532.27\n",
      "v/loss                                              |  3992.40\n",
      "q1/loss                                             |  9784.54\n",
      "q2/loss                                             |  9289.70\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |   293.30\n",
      "q1/grad                                             |   153.42\n",
      "q2/grad                                             |   180.28\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   694.64\n",
      "Env/Length/Episode (New)                            |   137.52\n",
      "Env/Reward/Episode (Last 50)                        |   721.50\n",
      "Env/Length/Episode (Last 50)                        |   142.30\n",
      "policy/loss                                         | -3072.31\n",
      "v/loss                                              |  6878.85\n",
      "q1/loss                                             | 13987.48\n",
      "q2/loss                                             | 15660.96\n",
      "policy/grad                                         |     0.00\n",
      "v/grad                                              |  -543.58\n",
      "q1/grad                                             |   513.08\n",
      "q2/grad                                             |   248.71\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   660.08\n",
      "Env/Length/Episode (New)                            |   130.61\n",
      "Env/Reward/Episode (Last 50)                        |   666.89\n",
      "Env/Length/Episode (Last 50)                        |   130.96\n",
      "policy/loss                                         | -3473.83\n",
      "v/loss                                              |  3134.29\n",
      "q1/loss                                             |  5797.38\n",
      "q2/loss                                             |  5057.80\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              |   -31.14\n",
      "q1/grad                                             |   -83.51\n",
      "q2/grad                                             |    79.82\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   733.04\n",
      "Env/Length/Episode (New)                            |   144.00\n",
      "Env/Reward/Episode (Last 50)                        |   695.57\n",
      "Env/Length/Episode (Last 50)                        |   137.42\n",
      "policy/loss                                         | -3405.94\n",
      "v/loss                                              |  5163.84\n",
      "q1/loss                                             |  8841.03\n",
      "q2/loss                                             |  7832.32\n",
      "policy/grad                                         |     0.01\n",
      "v/grad                                              | -1589.76\n",
      "q1/grad                                             |   -84.18\n",
      "q2/grad                                             |  -110.99\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   618.91\n",
      "Env/Length/Episode (New)                            |   122.78\n",
      "Env/Reward/Episode (Last 50)                        |   650.72\n",
      "Env/Length/Episode (Last 50)                        |   128.32\n",
      "policy/loss                                         | -3248.68\n",
      "v/loss                                              |  4410.82\n",
      "q1/loss                                             |  7596.03\n",
      "q2/loss                                             |  8929.96\n",
      "policy/grad                                         |     0.02\n",
      "v/grad                                              |   702.35\n",
      "q1/grad                                             |  -264.14\n",
      "q2/grad                                             |  -240.64\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   814.56\n",
      "Env/Length/Episode (New)                            |   163.68\n",
      "Env/Reward/Episode (Last 50)                        |   707.26\n",
      "Env/Length/Episode (Last 50)                        |   141.56\n",
      "policy/loss                                         | -3353.17\n",
      "v/loss                                              | 13482.08\n",
      "q1/loss                                             | 11647.98\n",
      "q2/loss                                             | 12810.77\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              | -1170.64\n",
      "q1/grad                                             |  -643.57\n",
      "q2/grad                                             |  -466.31\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   642.47\n",
      "Env/Length/Episode (New)                            |   127.53\n",
      "Env/Reward/Episode (Last 50)                        |   727.44\n",
      "Env/Length/Episode (Last 50)                        |   145.30\n",
      "policy/loss                                         | -3364.20\n",
      "v/loss                                              |  5880.93\n",
      "q1/loss                                             |  9575.09\n",
      "q2/loss                                             |  9772.31\n",
      "policy/grad                                         |    -0.02\n",
      "v/grad                                              | -1083.04\n",
      "q1/grad                                             |   400.80\n",
      "q2/grad                                             |   616.60\n",
      "--------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Env/Reward/Episode (New)                            |   629.32\n",
      "Env/Length/Episode (New)                            |   124.24\n",
      "Env/Reward/Episode (Last 50)                        |   639.45\n",
      "Env/Length/Episode (Last 50)                        |   126.58\n",
      "policy/loss                                         | -3358.70\n",
      "v/loss                                              |  4365.39\n",
      "q1/loss                                             | 12146.41\n",
      "q2/loss                                             | 12723.96\n",
      "policy/grad                                         |    -0.01\n",
      "v/grad                                              |  1036.07\n",
      "q1/grad                                             |  -399.66\n",
      "q2/grad                                             |  -600.27\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for batch in batcher.get_batches(MAX_STEPS, policy.get_action):\n",
    "    batch = batch.to_tensor().concat_batch()\n",
    "\n",
    "    ##### Calculate losses ######\n",
    "    q1_batch = q1_nn((batch.state_t, batch.action))\n",
    "    q2_batch = q2_nn((batch.state_t, batch.action))\n",
    "    v_batch = v_nn(batch.state_t)\n",
    "\n",
    "    dist = policy.create_dist(batch.state_t)\n",
    "    if REPAR:\n",
    "        action, pre_tanh_action = dist.rsample_with_pre()\n",
    "    else:\n",
    "        action, pre_tanh_action = dist.sample_with_pre()\n",
    "    log_prob = dist.log_prob_pre(pre_tanh_action).sum(-1, keepdim=True)\n",
    "\n",
    "    # Q loss\n",
    "    v_target_tp1 = v_nn_target(batch.state_tp1)\n",
    "    q_value_tp1 = U.estimators.td_target(rewards=batch.reward, dones=batch.done,\n",
    "                                         v_tp1=v_target_tp1, gamma=GAMMA)\n",
    "#     q_value_tp1 = batch.reward + (1 - batch.done) * GAMMA * v_target_tp1\n",
    "    q1_loss = F.mse_loss(q1_batch, q_value_tp1.detach())\n",
    "    q2_loss = F.mse_loss(q2_batch, q_value_tp1.detach())\n",
    "\n",
    "    # V loss    \n",
    "    q1_new_t = q1_nn((batch.state_t, action))\n",
    "    q2_new_t = q2_nn((batch.state_t, action))\n",
    "    q_new_t = torch.min(q1_new_t, q2_new_t)\n",
    "    next_value = q_new_t - log_prob\n",
    "    v_loss = F.mse_loss(v_batch, next_value.detach())\n",
    "\n",
    "    # Policy loss\n",
    "    # Using q1_new_t instead of minimum might be wrong, but it's what haarnoja uses in the code\n",
    "    if REPAR:        \n",
    "        p_loss = (log_prob - q1_new_t).mean()\n",
    "    else:\n",
    "        next_log_prob = q1_new_t - v_batch\n",
    "        p_loss = (log_prob * (log_prob - next_log_prob).detach()).mean()\n",
    "    # Policy regularization losses\n",
    "    mean_loss = 1e-3 * dist.loc.pow(2).mean()\n",
    "    log_std_loss = 1e-3 * dist.scale.log().pow(2).mean()\n",
    "    pre_tanh_loss = 0 * pre_tanh_action.pow(2).sum(1).mean()\n",
    "    # Combine all losses\n",
    "    p_loss += mean_loss + log_std_loss + pre_tanh_loss\n",
    "\n",
    "    ###### Optimize ######\n",
    "    q1_opt.zero_grad()\n",
    "    q1_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(q1_nn.parameters(), CLIP_GRAD)\n",
    "    q1_grad = U.mean_grad(q1_nn)\n",
    "    q1_opt.step()\n",
    "\n",
    "    q2_opt.zero_grad()\n",
    "    q2_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(q2_nn.parameters(), CLIP_GRAD)\n",
    "    q2_grad = U.mean_grad(q2_nn)\n",
    "    q2_opt.step()\n",
    "\n",
    "    v_opt.zero_grad()\n",
    "    v_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(v_nn.parameters(), CLIP_GRAD)\n",
    "    v_grad = U.mean_grad(v_nn)\n",
    "    v_opt.step()\n",
    "\n",
    "    p_opt.zero_grad()\n",
    "    p_loss.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(p_nn.parameters(), CLIP_GRAD)\n",
    "    p_grad = U.mean_grad(p_nn)\n",
    "    p_opt.step()\n",
    "\n",
    "    ###### Update target value network ######\n",
    "    U.copy_weights(from_nn=v_nn, to_nn=v_nn_target, weight=TARGET_UP_WEIGHT)\n",
    "\n",
    "    ###### Write logs ######\n",
    "    if batcher.num_steps % 4000 == 0 and batcher.runner.rewards:\n",
    "        batcher.write_logs(logger)    \n",
    "\n",
    "        logger.add_log('policy/loss', p_loss)\n",
    "        logger.add_log('v/loss', v_loss)\n",
    "        logger.add_log('q1/loss', q1_loss)\n",
    "        logger.add_log('q2/loss', q2_loss)\n",
    "\n",
    "        logger.add_log('policy/grad', p_grad)\n",
    "        logger.add_log('v/grad', v_grad)\n",
    "        logger.add_log('q1/grad', q1_grad)\n",
    "        logger.add_log('q2/grad', q2_grad)\n",
    "\n",
    "        logger.add_histogram('policy/log_prob', log_prob)\n",
    "        logger.add_histogram('policy/mean', dist.loc)\n",
    "        logger.add_histogram('policy/std', dist.scale.exp())\n",
    "        logger.add_histogram('v/value', v_batch)\n",
    "        logger.add_histogram('q1/value', q1_batch)\n",
    "        logger.add_histogram('q2/value', q2_batch)\n",
    "\n",
    "        logger.log(step=batcher.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
